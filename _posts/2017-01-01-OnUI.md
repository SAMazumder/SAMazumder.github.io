---
title:  "On Uniform Integrability"

mathjax: true

layout: post

categories: math, probability
---

## Uniform Integrability

#### Definition (Uniform Integrability)
A non-empty family $ \mathcal{X} \subset \mathcal{L}^{0} $ of random variables is said to be uniformly integrable (UI) if 

$$ \lim\limits_{n\to \infty}\left(\sup_{X\in \mathcal{X}} \mathbb{E} \left[ |X| \mathbb{1}_{\{|X|\geq n\}}\right]\right) = 0.$$

That is, for integrable random variables, the far tails contribute very little to the expectation. 


#### Proposition

$$ \lim\limits_{n\to \infty}\left( \mathbb{E} \left[|X|\mathbb{1}_{\{|X|\geq n\}}\right]\right) = 0 \text{ if and only if } X \in \mathcal{L}^1$$

#### Proof

Suppose $ X \in \mathcal{L}^{1} $, that is, $ \mathbb{E}[|X|] < \infty $. 

Now, 

$$ n \mathbb{P}\{|X| \geq n\} = \mathbb{E} [n \mathbb{1}_{\{|X| \geq n\}}] \leq \mathbb{E} [|X|\mathbb{1}_{\{|X| \geq n\}}] \leq \mathbb{E}[|X|] $$

for every $ n \in \mathbb{N} $. 


Hence, 

$$ \mathbb{P}\{|X| = \infty\}=\lim\limits_{n\to \infty}\mathbb{P}\{|X| \geq n\} \leq \lim\limits_{n\to \infty} \frac{1}{n} \mathbb{E}[|X|] = 0. $$ 

Since we are taking an integral over a null set, $ \mathbb{P}\{|X| = \infty\} = 0 $, we have, 

$$ \lim\limits_{n\to \infty} \mathbb{E}[\mathbb{1}_{\{|X| \geq n\}}|X|] = \lim\limits_{n\to \infty} \int_{\{|X| \geq n\}} |X| d\mathbb{P} = 0. $$

To show the other direction, toward a contradiction, suppose  $\lim\limits_{n\to \infty}\left( \mathbb{E} \left[|X|\mathbb{1}_{\{|X|\geq n\}}\right]\right) = 0 $ and $X \not\in \mathcal{L}^1. $

Notice that,

$$ \mathbb{E}[|X|]
    =\\mathbb{E}[|X|\mathbb{1}_{\{|X| \geq n\}}] +  \mathbb{E}[|X|\mathbb{1}_{\{|X| < n\}}] \text{ for every } n $$

$$ ~ \leq \mathbb{E}[|X|\mathbb{1}_{\{|X| \geq n\}}] + n\mathbb{P}\{|X| \leq n\} $$

$$ ~ \leq \mathbb{E}[|X|\mathbb{1}_{\{|X| \geq n\}}] + n_0 \text{ for some } n_0 \in \mathbb{R}. $$

Taking limits over $ n $, we have $ \infty = \E [\abs{X}] \leq n_0 $, which is a contradiction. Thus, we have $ \lim\limits_{n\to \infty}\left( \mathbb{E} \left[|X|\mathbb{1}_{\{|X|\geq n\}}\right]\right) = 0$ if and only if $ X \in \mathcal{L}^1$.

Some immediate observations from the above definition are as follows

#### Observations
1. The dominating condition of Lebesgue dominating convergence implies uniform integrability of $ (X_n) $.
    
    #### Proof
    Notice, $ \mathbb{E} \left[\sup_{X\in \mathcal{X}}|X|\mathbb{1}_{\{|X|\geq k\}}\right] \leq \mathbb{E} \left[Y\mathbb{1}_{\{|Y|\geq k\}}\right] $ by the monotonicity of expectation. By the proposition above, the proof follows. 
    
    
2. Since $ \mathbb{P}(\Omega) =1 $ and when $\mathbb{P}$ a complete measure, any $ X \in \mathcal{L}^1 $ is trivially uniformly integrable. 
    
    #### Proof
    Recall that for $ X \in \mathcal{L}^{1 }$, we have that $ |X| $ is finite a.e. Set $ X_n := \mathbb{1}_{\{|X| \geq n \}}|X| $, then we clearly have $ X_n \leq |X| \leq M_0 $ a.e., and the proof follows from the first observation above.  

3. $ \mathcal{X} $ is a class of uniformly integrable random variables, if and only if
    1. $ \sup_{X\in \mathcal{X}}\mathbb{E}[|X|] < \infty$, and
    2. for every $ \varepsilon > 0 $ there exists $ \delta > 0 $ such that, for every $ A \in \mathcal{F} $ such that $ \mathbb{P}(A) < \delta $ and every $ X \in \mathcal{X} $, we have $ \mathbb{E}\left[\mathbb{1}_{A}|A|\right] < \varepsilon $.
    
    #### Proof
    Fix $ X \in \mathcal{X} $, such that $ \mathcal{X} $ is a class of uniformly integrable random variables. Indeed, $ \mathbb{E}[|X|] =  \mathbb{E} \left[|X|\mathbb{1}_{\{|X|\geq k\}}\right] + \mathbb{E} \left[|X|\mathbb{1}_{|X|< k\}}\right] \leq \mathbb{E} \left[|X|\mathbb{1}_{\{|X|\geq k\}}\right] + k_0.$ And again bythe above proposition, it follows that $\sup_{X\in \mathcal{X}\mathbb{E}[|X|] < \infty$.
        
        Now, 
        \begin{align*}
            \E[\abs{X}\1_A] 
            &= \E[\abs{X} \1_{A \cap \{\abs{X}\geq k\}} ] + \E[\abs{X} \1_{A \cap \{\abs{X}< k\}} ]\\
            &\leq \E[\abs{X} \1_{A \cap \{\abs{X}\geq k\}} ] + k_0 \P(A).
        \end{align*}
        For sufficiently large $k$, $\E[\abs{X_n} \1_{A \cap \{\abs{X_n}\geq k\}} ] \leq \E[\abs{X_n} \1_{\{\abs{X_n}\geq k\}} ] < \frac{\varepsilon}{2}$.  By putting $\P(A)<\delta = \frac{\varepsilon}{2k_0}$, we have the desired inequality, $ \E[\abs{X_n}\1_A] \leq \varepsilon$.
        
        For the other direction, suppose $\sup_{X\in \cX}\E[\abs{X}] < +\infty$ and $\lim_{\P(A) \to 0} \E[\abs{X}\1_A] = 0$ holds for some collection of random variables $\cX$ and put $M:=\sup_{X \in \cX}\E[\abs{X}]$. By Chebshev, we have:
            \begin{align*}
                \P(\{\abs{X} \geq n\}) \leq \frac{1}{n}\E[\abs{X}] \leq \frac{M}{n}
            \end{align*}
            For large enough $n$ from $(ii)$, we can put $\frac{M}{n} < \delta = \varepsilon$, then $\sup_{X\in \cX}\E[\abs{X}\1_{\{\abs{X} \geq n\}}] \leq \sup_{X\in \cX}\E[\abs{X}] < \varepsilon$ as desired.

    \end{proof}
\end{enumerate}
\end{observation}

\begin{theorem}[Uniform Integrability Criterion]\label{thrm:UIcrit} For a sequence $(X_n) \in \L_{1}$ such that $X_n \overset{\as}{\to} X$, the following are equivalent:
\begin{enumerate}
    \item Each $X_n$ is uniformly integrable,
    \item $X \in \L^1$ and $X_n \overset{\norm{\cdot}_{\L_{1}}}{\to} X$,
    \item $\norm{X_n} \to \norm{X}$.
\end{enumerate}
Any of the above conditions imply $\E[X_n] \to \E[X].$
\end{theorem}

The reader can find the proof of the above in~\cite{modapp}. We can weaken the above as follows:


\begin{theorem}[Vitali's Convergence Theorem]\label{thrm:BetterLDC} For a class of uniformly integrable random variables,$(X_n) \in \cX$, $X_n \overset{\P}{\to} X$ if and only if $X \in L^{1}$ and $X_n \overset{\norm{\cdot}_{\L_{1}}}{\to} X$.
\end{theorem}

Indeed, together~\cref{thrm:UIcrit,thrm:BetterLDC} are necessary and sufficient conditions for $\L^{1}$ convergence. The above is a \emph{Generalization of Lebesgue's Dominated Convergence Theorem.}

\begin{proof}
The necessary condition holds in general by Chebychev that for a fixed $p \geq 1$, we have $\P(\{\abs{X_n - X}^{p} \}> \varepsilon)  \leq \frac{1}{\varepsilon}\E[\abs{X_n -X}^{p}]$. For the sufficient condition, consider that if $X_n \overset{\P}{\to} X$, there exists a subsequence $X_{n_k} \overset{\as}{\to}X$. By an application of Fatou's Lemma as in \cref{Cor:FatousLemma}, we have 
$$\E[\abs{X}] = \E[\liminf_{k\to \infty}\abs{X_{n_k}}] \leq \liminf_{k\to \infty}\E[\abs{X_{n_k}}] \leq \sup_{X_n \in \cX}\E[\abs{X_n}] < \infty,$$
since each $\cX$ is a uniformly integrable set and thus bounded in $\L^{1}$. Hence $X \in \L^{1}$ and thus $\{\abs{X_n - X}\}$ is also a uniformly integrable set by \cref{Obs:UI}.

Now to show convergence in norm, notice that since there exists a subsequence $X_{k_n} \overset{\as}{\to}X$, we have a further subsequence $X_{n_k'} \overset{\P}{\to} X$. Hence there exists another further subsequence $X_{n_k''} \overset{\as}{\to}X$. By the first part of the sufficient condition, we have $X_{n_k''} \overset{\norm{\cdot}_{\L^{1}}}{\to}X$. But notice that the limit $X$ does not depend on the subsequence. Hence by reductio ad absurdum, we have that $X_n$ itself converges to $X$ in norm. 

\end{proof} 
